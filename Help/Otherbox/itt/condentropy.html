<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of condentropy</title>
  <meta name="keywords" content="condentropy">
  <meta name="description" content="CONDENTROPY conditional entropy between two categorical-discrete variables">
  <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
  <meta name="generator" content="m2html &copy; 2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
    <link type="text/css" rel="stylesheet" href="../../m2html.css">
  <script type="text/javascript">
    if (top.frames.length == 0) { top.location = "../../index.html"; };
  </script>
</head>
<body>
<a name="_top"></a>
<!-- # Otherbox --><!-- menu.html itt -->
<h1>condentropy
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>CONDENTROPY conditional entropy between two categorical-discrete variables</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function h = condentropy(x,y) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment" style="background-image:url(../../brain.png)"><pre class="comment">CONDENTROPY conditional entropy between two categorical-discrete variables
   Given (X,Y), the conditional entropy H(Y|X) quantifies the remaining 
   entropy (i.e. uncertainty) of a variable Y given that the value of the 
   second variable X is known [1]

   Joaquin Goñi &lt;jgoni@unav.es&gt; &amp; 
   Iñigo Martincorena &lt;imartincore@alumni.unav.es&gt;
   University of Navarra - Dpt. of Physics and Applied Mathematics &amp;
   Centre for Applied Medical Research. Pamplona (Spain).
   
   December 13th, 2007. Information Theory Toolbox v1.0

   h = condentropy(x,y) for x,y being column vectors with categorical 
   measures, it computes the conditional entropy [1][2] H(Y|X) found on Y 
   given X.
   
   x,y variables must be column vectors with categorical data. The number
   of categories per variable can be different.

   Example:

       x = [1;2;2;2;0;0;1;0;1;2];
       y = [1;2;2;2;2;1;0;2;1;0];
       h = condentropy(x,y) returns H(y|x)

   Citation:

   If you use them for your academic research work,please kindly cite this 
   toolbox as: 
   Joaquin Goñi, Iñigo Martincorena. Information Theory Toolbox v1.0.  
   University of Navarra - Dpt. of Physics and Applied Mathematics &amp; 
   Centre for Applied Medical Research. Pamplona (Spain).

   References
   [1] C. E. Shannon, A mathematical theory of communication, Bell System 
   Technical Journal, vol. 27, pp. 379-423 and 623-656, July and October, 
   1948.
   [2] http://en.wikipedia.org/wiki/Conditional_entropy</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment" style="background-image:url(../../brain.png)">
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
</ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="mutualinformation.html" class="code" title="function [mi,estSignif] = mutualinformation(x,y,numSim)">mutualinformation</a>	MUTUALINFORMATION mutual information for categorical-discrete data with</li></ul>
</div>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment" style="background-image:url(../../brain.png)"><pre>0001 <span class="comment">%CONDENTROPY conditional entropy between two categorical-discrete variables</span>
0002 <span class="comment">%   Given (X,Y), the conditional entropy H(Y|X) quantifies the remaining</span>
0003 <span class="comment">%   entropy (i.e. uncertainty) of a variable Y given that the value of the</span>
0004 <span class="comment">%   second variable X is known [1]</span>
0005 <span class="comment">%</span>
0006 <span class="comment">%   Joaquin Goñi &lt;jgoni@unav.es&gt; &amp;</span>
0007 <span class="comment">%   Iñigo Martincorena &lt;imartincore@alumni.unav.es&gt;</span>
0008 <span class="comment">%   University of Navarra - Dpt. of Physics and Applied Mathematics &amp;</span>
0009 <span class="comment">%   Centre for Applied Medical Research. Pamplona (Spain).</span>
0010 <span class="comment">%</span>
0011 <span class="comment">%   December 13th, 2007. Information Theory Toolbox v1.0</span>
0012 <span class="comment">%</span>
0013 <span class="comment">%   h = condentropy(x,y) for x,y being column vectors with categorical</span>
0014 <span class="comment">%   measures, it computes the conditional entropy [1][2] H(Y|X) found on Y</span>
0015 <span class="comment">%   given X.</span>
0016 <span class="comment">%</span>
0017 <span class="comment">%   x,y variables must be column vectors with categorical data. The number</span>
0018 <span class="comment">%   of categories per variable can be different.</span>
0019 <span class="comment">%</span>
0020 <span class="comment">%   Example:</span>
0021 <span class="comment">%</span>
0022 <span class="comment">%       x = [1;2;2;2;0;0;1;0;1;2];</span>
0023 <span class="comment">%       y = [1;2;2;2;2;1;0;2;1;0];</span>
0024 <span class="comment">%       h = condentropy(x,y) returns H(y|x)</span>
0025 <span class="comment">%</span>
0026 <span class="comment">%   Citation:</span>
0027 <span class="comment">%</span>
0028 <span class="comment">%   If you use them for your academic research work,please kindly cite this</span>
0029 <span class="comment">%   toolbox as:</span>
0030 <span class="comment">%   Joaquin Goñi, Iñigo Martincorena. Information Theory Toolbox v1.0.</span>
0031 <span class="comment">%   University of Navarra - Dpt. of Physics and Applied Mathematics &amp;</span>
0032 <span class="comment">%   Centre for Applied Medical Research. Pamplona (Spain).</span>
0033 <span class="comment">%</span>
0034 <span class="comment">%   References</span>
0035 <span class="comment">%   [1] C. E. Shannon, A mathematical theory of communication, Bell System</span>
0036 <span class="comment">%   Technical Journal, vol. 27, pp. 379-423 and 623-656, July and October,</span>
0037 <span class="comment">%   1948.</span>
0038 <span class="comment">%   [2] http://en.wikipedia.org/wiki/Conditional_entropy</span>
0039 
0040 <a name="_sub0" href="#_subfunctions" class="code">function h = condentropy(x,y)</a>
0041 
0042 <span class="keyword">if</span> ~(isvector(x) &amp;&amp; isvector(y))    <span class="comment">%x,y must be one-dimensional vectors</span>
0043     disp(<span class="string">'Error: input data must be one dimensional vectors'</span>)
0044     h = nan;
0045 
0046 <span class="keyword">elseif</span> size(x)~=size(y) <span class="comment">%vectors must have the same size</span>
0047     disp(<span class="string">'Error: input vectors must have the same size'</span>)
0048     h = nan;
0049 <span class="keyword">else</span>
0050     valuesX = unique(x);    <span class="comment">%different values found in vector x</span>
0051     valuesY = unique(y);    <span class="comment">%different values found in vector y</span>
0052     
0053     numValues = numel(x);   <span class="comment">%number of elements contained on each vector</span>
0054     h = 0;  <span class="comment">%initial value of conditional entropy</span>
0055     
0056     <span class="keyword">for</span> i = valuesX'  <span class="comment">%for each different value found in x</span>
0057         
0058         iOcurrences = find(x==i);   <span class="comment">%ocurrences in x being i</span>
0059         numOcurrences = numel(iOcurrences); <span class="comment">%number of ocurrences in x being i</span>
0060         pi = numOcurrences / numValues;  <span class="comment">%probability p(i)</span>
0061         
0062         <span class="keyword">for</span> j = valuesY'   <span class="comment">%for each different value found in y</span>
0063             jOcurrences = numel(find(y(iOcurrences)==j));   <span class="comment">%ocurrences in subset of y being j</span>
0064             <span class="keyword">if</span> jOcurrences &gt; 0  <span class="comment">%if there where at at least one ocurrence</span>
0065                 pjcondi = jOcurrences / numOcurrences;  <span class="comment">%partial conditional entropy for specific values of x and y</span>
0066                 h = h - pi*pjcondi*log2(pjcondi);   <span class="comment">%cumulative of partial conditional entropies</span>
0067             <span class="keyword">end</span>
0068         <span class="keyword">end</span> 
0069     <span class="keyword">end</span>
0070 <span class="keyword">end</span>
0071 
0072             
0073</pre></div>
<hr><address>Copyright (C) 2008-2010 Pu Jiangbo @ Britton Chance Center for Biomedical Photonics<br/>Generated on Fri 22-Jun-2012 16:47:48</address>
</body>
</html>