<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of ConditionalEntropy</title>
  <meta name="keywords" content="ConditionalEntropy">
  <meta name="description" content="ConditionalEntropy: Calculates conditional entropy (in bits) of Y, given X">
  <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
  <meta name="generator" content="m2html &copy; 2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
    <link type="text/css" rel="stylesheet" href="../../m2html.css">
  <script type="text/javascript">
    if (top.frames.length == 0) { top.location = "../../index.html"; };
  </script>
</head>
<body>
<a name="_top"></a>
<!-- # Otherbox --><!-- menu.html nmi -->
<h1>ConditionalEntropy
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>ConditionalEntropy: Calculates conditional entropy (in bits) of Y, given X</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="box"><strong>function H = ConditionalEntropy(Y,X) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment" style="background-image:url(../../brain.png)"><pre class="comment"> ConditionalEntropy: Calculates conditional entropy (in bits) of Y, given X
 by Will Dwinnell

 H = ConditionalEntropy(Y,X)

 H  = calculated entropy of Y, given X (in bits)
 Y  = dependent variable (column vector)
 X  = independent variable(s)
 
 Note: requires 'Entropy' and 'MutualInformation' functions

 Example: 'X' (1 bit of entropy), 'Y' (2 bits of entropy)
   Given 'X', 'Y's conditional entropy is 1 bit.

 Note: Estimated entropy values are slightly less than true, due to
 finite sample size.

 Y = ceil(4 * rand(1e3,1));  X = double(Y &lt;= 2);
 ConditionalEntropy(Y,X)

 Last modified: Nov-12-2006</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment" style="background-image:url(../../brain.png)">
This function calls:
<ul style="list-style-image:url(../../matlabicon.gif)">
<li><a href="Entropy.html" class="code" title="function H = Entropy(X)">Entropy</a>	Entropy: Returns entropy (in bits) of each column of 'X'</li><li><a href="MutualInformation.html" class="code" title="function I = MutualInformation(X,Y);">MutualInformation</a>	MutualInformation: returns mutual information (in bits) of the 'X' and 'Y'</li></ul>
This function is called by:
<ul style="list-style-image:url(../../matlabicon.gif)">
</ul>
</div>
<!-- crossreference -->



<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../up.png"></a></h2>
<div class="fragment" style="background-image:url(../../brain.png)"><pre>0001 <span class="comment">% ConditionalEntropy: Calculates conditional entropy (in bits) of Y, given X</span>
0002 <span class="comment">% by Will Dwinnell</span>
0003 <span class="comment">%</span>
0004 <span class="comment">% H = ConditionalEntropy(Y,X)</span>
0005 <span class="comment">%</span>
0006 <span class="comment">% H  = calculated entropy of Y, given X (in bits)</span>
0007 <span class="comment">% Y  = dependent variable (column vector)</span>
0008 <span class="comment">% X  = independent variable(s)</span>
0009 <span class="comment">%</span>
0010 <span class="comment">% Note: requires 'Entropy' and 'MutualInformation' functions</span>
0011 <span class="comment">%</span>
0012 <span class="comment">% Example: 'X' (1 bit of entropy), 'Y' (2 bits of entropy)</span>
0013 <span class="comment">%   Given 'X', 'Y's conditional entropy is 1 bit.</span>
0014 <span class="comment">%</span>
0015 <span class="comment">% Note: Estimated entropy values are slightly less than true, due to</span>
0016 <span class="comment">% finite sample size.</span>
0017 <span class="comment">%</span>
0018 <span class="comment">% Y = ceil(4 * rand(1e3,1));  X = double(Y &lt;= 2);</span>
0019 <span class="comment">% ConditionalEntropy(Y,X)</span>
0020 <span class="comment">%</span>
0021 <span class="comment">% Last modified: Nov-12-2006</span>
0022 
0023 <a name="_sub0" href="#_subfunctions" class="code">function H = ConditionalEntropy(Y,X)</a>
0024 
0025 <span class="comment">% Axiom of information theory</span>
0026 H = <a href="Entropy.html" class="code" title="function H = Entropy(X)">Entropy</a>(Y) - <a href="MutualInformation.html" class="code" title="function I = MutualInformation(X,Y);">MutualInformation</a>(X,Y);
0027 
0028 
0029 <span class="comment">% God bless Claude Shannon.</span>
0030 
0031 <span class="comment">% EOF</span>
0032 
0033</pre></div>
<hr><address>Copyright (C) 2008-2010 Pu Jiangbo @ Britton Chance Center for Biomedical Photonics<br/>Generated on Fri 22-Jun-2012 16:47:48</address>
</body>
</html>