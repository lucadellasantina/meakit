%KLDIVERGENCE redundancy measure for categorical-discrete data
%   KL = KLDIVERGENCE(X,Y) returns the redundancy between the two column
%   vectors X and Y containing categorical measures.
%
%   KLDIVERGENCE [1] also known as information divergence, information gain, 
%   or relative entropy is a measure of the difference between two 
%   probability distributions: from a "true" probability distribution P to 
%   an arbitrary probability distribution Q. 
%
%   It is important to remark that the implementation introduced in this 
%   script allows to compute the KL divergence directly on two column 
%   vectors containing categorical data instead of requiring their 
%   probability distributions as inputs. The number of categories (distinct
%   values) per vector must be the same (but not the same values ).
%
%   Joaquin Go単i <jgoni@unav.es> & 
%   I単igo Martincorena <imartincore@alumni.unav.es>
%   University of Navarra - Dpt. of Physics and Applied Mathematics &
%   Centre for Applied Medical Research. Pamplona (Spain).
% 
%   December 13th, 2007. Information Theory Toolbox v1.0
%
%   Example:
%   
%    x = [1;2;2;2;0;0;1;0;1;2];
%    y = [1;2;2;2;2;1;0;2;1;0];
%    kl = kldivergence(x,y);
%
%   Citation:
%
%   If you use them for your academic research work,please kindly cite this 
%   toolbox as: 
%   Joaquin Go単i, I単igo Martincorena. Information Theory Toolbox v1.0.  
%   University of Navarra - Dpt. of Physics and Applied Mathematics & 
%   Centre for Applied Medical Research. Pamplona (Spain).
%
%   References
%   [1] Kullback, S., and Leibler, R. A., 1951, On information and 
%   sufficiency, Annals of Mathematical Statistics 22: 79-86.
%   [2] http://en.wikipedia.org/wiki/Kullback-Leibler_divergence

function kl = kldivergence (x,y)

if ~(isvector(x) && isvector(y))
    disp('Error: input data must be one dimensional vectors')
    mi = nan;
    estSignif = nan;

elseif size(x)~=size(y) %vectors must have the same size
    disp('Error: input vectors must have the same size')
    h = nan;
    estSignif = nan;
elseif numel(unique(x))~=numel(unique(y))
    disp('Range of discretizations must be the same for both vectors')
else

valuesX = unique(x);
valuesY = unique(y);
numValues = numel(x);
distinctValues = numel(unique(x));
kl = 0;

for i=1:distinctValues
    ipOcurrences = numel(find(x==valuesX(i)));
    iqOcurrences = numel(find(y==valuesY(i)));
    pi = ipOcurrences/numValues;
    qi = iqOcurrences/numValues;
    klCurrent = pi*log2(pi/qi);
    kl=kl + klCurrent;
end
end